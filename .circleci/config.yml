version: 2.1

orbs:
  slack: circleci/slack@4.4.2

slack-notify-deploy-fail: &slack-notify-deploy-fail
      event: fail
      mentions: '@andrenoberto'
      template: basic_fail_1

slack-notify-deploy-success: &slack-notify-deploy-success
      event: pass
      template: success_tagged_deploy_1

slack-notify-job-fail: &slack-notify-job-fail
      event: fail
      mentions: '@andrenoberto'
      template: basic_fail_1

slack-notify-job-success: &slack-notify-job-success
      event: pass
      template: basic_success_1

alpine-image: &alpine-image
  docker:
    - image: alpine:latest

aws-cli: &aws-cli
  docker:
    - image: amazon/aws-cli

default-image: &default-image
  docker:
    - image: python:3.7-alpine3.12

nodejs-image: &nodejs-image
  docker:
    - image: circleci/node:13.8.0

commands:
  destroy-environment:
    description: Destroy back-end and front-end cloudformation stacks given a workflow ID.
    parameters:
      workflowId:
        type: string
    steps:
      - run: 
          name: Destroy stacks
          command: |
            export BUCKET_EXISTS=$(aws s3 ls s3://s3-bucket-udapeople-<< parameters.workflowId >> 2>&1 | grep -c 'index.html')
            if [ $BUCKET_EXISTS -eq 1 ]
            then
              aws s3 rm s3://s3-bucket-udapeople-<< parameters.workflowId >> --recursive
            fi
            aws cloudformation delete-stack --stack-name udapeople-backend-<< parameters.workflowId >>
            aws cloudformation delete-stack --stack-name udapeople-frontend-<< parameters.workflowId >>
          when: on_fail
  install-aws-cli:
    description: Install AWS cli using apt-get
    steps:
      - run:
          name: Install AWS cli
          command: |
            sudo apt-get install -y python3 python3-pip
            pip3 install --upgrade pip
            pip3 install awscli
  install-dependencies:
    description: Install all missing dependencies
    steps:
      - run:
          name: Install all missing dependencies
          command: |
            yum clean all
            yum update -y
            yum install tar gzip jq -y
  save-key-to-mem-stash:
    description: Saves a key into MemStash
    parameters:
      keyName:
        type: string
      keyValue:
        type: string
    steps:
      - run:
          name: Save key to MemStash
          command: |
            curl -H "Content-Type: text/plain" \
              -H "token: ${MEMSTASH_TOKEN}" --request PUT \
              --data "<< parameters.keyValue >>" https://api.memstash.io/values/<< parameters.keyName >>
          when: on_success
  retrieve-key-from-mem-stash:
    description: Retrieves a key stored in MemStash
    parameters:
      keyName:
        type: string
    steps:
      - run:
          name: Retrieve key from MemStash
          command: |
            echo 'export << parameters.keyName >>=$(curl -H "token: ${MEMSTASH_TOKEN}" --request GET https://api.memstash.io/values/<< parameters.keyName >>)' >> $BASH_ENV
  revert-migrations:
    description: Revert the last migration if successfully run in the current workflow.
    steps:
      - checkout
      - retrieve-key-from-mem-stash:
          keyName: MIGRATION_${CIRCLE_WORKFLOW_ID:0:7}
      - run:
          name: Revert migrations
          command: |
            source $BASH_ENV
            if [ MIGRATION_${CIRCLE_WORKFLOW_ID:0:7} -eq 0 ]
            then
              cd backend
              npm install
              npm run migrations:revert
            fi
          when: on_fail
            
jobs:
  build-frontend:
    <<: *nodejs-image
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Build front-end
          command: |
            cd frontend
            npm install
            npm run build:prod
      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-build
      - slack/notify: *slack-notify-job-fail

  build-backend:
    <<: *nodejs-image
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: Back-end build
          command: |
            cd backend
            npm install
            npm run build
      - save_cache:
          paths: [backend/node_modules]
          key: backend-build
      - persist_to_workspace:
          root: ~/
          paths:
            - project/backend/dist/
      - slack/notify: *slack-notify-job-fail

  test-frontend:
    <<: *nodejs-image
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Front-end unit tests
          command: |
            cd frontend
            npm run test
      - slack/notify: *slack-notify-job-fail
                
  test-backend:
    <<: *nodejs-image
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: Back-end unit tests
          command: |
            cd backend
            npm run test
      - slack/notify: *slack-notify-job-fail
            
  scan-frontend:
    <<: *nodejs-image
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-build]
      - run:
          name: Scan for vulnerable packages
          command: |
            cd frontend
            npm audit --audit-level=critical
      - slack/notify: *slack-notify-job-fail

  scan-backend:
    <<: *nodejs-image
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: Scan for vulnerable packages
          command: |
            cd backend
            npm audit --audit-level=critical
      - slack/notify: *slack-notify-job-fail

  deploy-infrastructure:
    <<: *aws-cli
    steps:
      - checkout
      - install-dependencies
      - run:
          name: Ensure database infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/database.yml \
              --tags project=udapeople \
              --stack-name udapeople-database \
              --parameter-overrides DatabaseUsername=${TYPEORM_USERNAME} DatabasePassword=${TYPEORM_PASSWORD}
          no_output_timeout: 20m
      - run:
          name: Ensure back-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/backend.yml \
              --tags project=udapeople \
              --stack-name udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7} \
              --parameter-overrides ID=${CIRCLE_WORKFLOW_ID:0:7} \
                  ImageID=${UBUNTU_AMI_ID} KeyName=${SSH_KEY_NAME} \
                  SSHCidrIp=${DEVELOPER_JUMPBOX_CIDR_IP} WebCidrIp=${WEB_CIDR_IP} \
                  DatabaseUsername=${TYPEORM_USERNAME} DatabasePassword=${TYPEORM_PASSWORD}
          no_output_timeout: 20m
      - save-key-to-mem-stash:
          keyName: BACKEND_PUBLIC_DNS_NAME
          keyValue: $(aws ec2 describe-instances --query 'Reservations[*].Instances[*].PublicDnsName' --filters "Name=tag:Name,Values=udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" --output text)
      - save-key-to-mem-stash:
          keyName: TYPEORM_PORT
          keyValue: $(aws cloudformation list-exports --query "Exports[?Name==\`DatabasePort\`].Value" --no-paginate --output text)
      - save-key-to-mem-stash:
          keyName: TYPEORM_HOST
          keyValue: $(aws cloudformation list-exports --query "Exports[?Name==\`DatabaseHost\`].Value" --no-paginate --output text)
      - run:
          name: Ensure front-end infrastructure exist
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=udapeople \
              --stack-name udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7} \
              --parameter-overrides ID=${CIRCLE_WORKFLOW_ID:0:7}
      - retrieve-key-from-mem-stash:
          keyName: BACKEND_PUBLIC_DNS_NAME
      - run:
          name: Add back-end ip to ansible inventory
          command: |
            source $BASH_ENV
            echo $BACKEND_PUBLIC_DNS_NAME >> .circleci/ansible/inventory.txt
      - persist_to_workspace:
          root: ~/
          paths:
            - project/.circleci/ansible/inventory.txt
      - destroy-environment:
          workflowId: ${CIRCLE_WORKFLOW_ID:0:7}
      - slack/notify: *slack-notify-deploy-fail

  deploy-prometheus:
    <<: *aws-cli
    steps:
      - checkout
      - run:
          name: Ensure Prometheus infrastructure exists
          command: |
            aws cloudformation deploy \
            --template-file .circleci/files/prometheus.yml \
            --stack-name prometheus
            --parameter-overrides ImageID=${UBUNTU_AMI_ID} \
                  KeyName=${SSH_KEY_NAME} \
                  SSHCidrIp=${DEVELOPER_JUMPBOX_CIDR_IP}
      - save-key-to-mem-stash:
          keyName: PROMETHEUS_PUBLIC_DNS_NAME
          keyValue: $(aws ec2 describe-instances --query 'Reservations[*].Instances[*].PublicDnsName' --filters 'Name=tag:Name,Values=Prometheus' --output text)
      - run:
          name: Prometheus rollback
          command: aws cloudformation delete-stack --stack-name prometheus
          when: on_fail
      - slack/notify: *slack-notify-deploy-fail

  configure-infrastructure:
    <<: *default-image
    steps:
      - add_ssh_keys:
          fingerprints: ["ba:cf:19:36:76:06:84:e1:c6:bd:b3:96:c1:f1:d6:f4"]
      - checkout
      - attach_workspace:
          at: ~/
      - run:
          name: Install gettext
          command: apk add --update gettext
      - run:
          name: Install Ansible
          command: apk add --update ansible
      - run:
          name: Install Curl
          command: apk add --update curl
      - retrieve-key-from-mem-stash:
          keyName: TYPEORM_HOST
      - retrieve-key-from-mem-stash:
          keyName: TYPEORM_PORT
      - run:
          name: Configure server
          command: |
            source $BASH_ENV
            envsubst < .circleci/ansible/roles/configure-server/files/.env.template > .circleci/ansible/roles/configure-server/files/.env
            ansible-playbook -i .circleci/ansible/inventory.txt .circleci/ansible/configure-server.yml
      - destroy-environment:
          workflowId: ${CIRCLE_WORKFLOW_ID:0:7}
      - slack/notify: *slack-notify-deploy-fail

  configure-prometheus:
    <<: *default-image
    steps:
      - add_ssh_keys:
          fingerprints: ["ba:cf:19:36:76:06:84:e1:c6:bd:b3:96:c1:f1:d6:f4"]
      - checkout
      - run:
          name: Install AWS cli
          command: |
            apk add --no-cache python3 py3-pip
            pip3 install --upgrade pip
            pip3 install awscli
      - run:
          name: Install gettext
          command: apk add --update gettext
      - run:
          name: Install Ansible
          command: apk add --update ansible
      - run:
          name: Install Curl
          command: apk add --update curl
      - retrieve-key-from-mem-stash:
          keyName: PROMETHEUS_PUBLIC_DNS_NAME
      - run:
          name: Update infrastructure
          command: |
            source $BASH_ENV
            echo $PROMETHEUS_PUBLIC_DNS_NAME >> .circleci/prometheus/inventory.txt
            envsubst < .circleci/prometheus/roles/install-prometheus/files/prometheus.template.yml > .circleci/prometheus/roles/install-prometheus/files/prometheus.yml
            envsubst < .circleci/prometheus/roles/install-prometheus/files/alertmanager.template.yml > .circleci/prometheus/roles/install-prometheus/files/alertmanager.yml
            ansible-playbook -i .circleci/prometheus/inventory.txt .circleci/prometheus/main.yml
      - slack/notify: *slack-notify-deploy-fail

  run-migrations:
    <<: *nodejs-image
    steps:
      - checkout
      - restore_cache:
          keys: [backend-build]
      - run:
          name: Install gettext
          command: sudo apt-get install -y gettext
      - run:
          name: Install Curl
          command: sudo apt-get install -y curl
      - retrieve-key-from-mem-stash:
          keyName: TYPEORM_HOST
      - retrieve-key-from-mem-stash:
          keyName: TYPEORM_PORT
      - run:
          name: Run migrations
          command: |
            source $BASH_ENV
            cp -R backend .circleci/ansible/roles/deploy-backend/files
            envsubst < backend/.env.template > backend/.env
            cd backend/
            if npm run migrations | grep "has been executed successfully"
            then
              echo 1 >> result.txt
            else
              echo 0 >> result.txt
            fi
      - save-key-to-mem-stash:
          keyName: MIGRATION_${CIRCLE_WORKFLOW_ID:0:7}
          keyValue: $(cat result.txt)
      - revert-migrations
      - slack/notify: *slack-notify-job-fail

  deploy-frontend:
    <<: *nodejs-image
    steps:
      - checkout
      - install-aws-cli
      - retrieve-key-from-mem-stash:
          keyName: BACKEND_PUBLIC_DNS_NAME
      - run:
          name: Build and Deploy frontend objects
          command: |
            source $BASH_ENV
            export API_URL="http://${BACKEND_PUBLIC_DNS_NAME}:3030"
            cd frontend
            npm install
            npm run build:prod
            cd dist
            aws s3 sync . s3://s3-bucket-udapeople-${CIRCLE_WORKFLOW_ID:0:7}
      - destroy-environment:
          workflowId: ${CIRCLE_WORKFLOW_ID:0:7}
      - revert-migrations
      - slack/notify: *slack-notify-job-fail
                    
  deploy-backend:
    <<: *default-image
    steps:
      - add_ssh_keys:
          fingerprints: ["ba:cf:19:36:76:06:84:e1:c6:bd:b3:96:c1:f1:d6:f4"]
      - checkout
      - attach_workspace:
          at: ~/
      - run:
          name: Install gettext
          command: apk add --update gettext
      - run:
          name: Install Ansible
          command: apk add --update ansible
      - run:
          name: Install Curl
          command: apk add --update curl
      - retrieve-key-from-mem-stash:
          keyName: TYPEORM_HOST
      - retrieve-key-from-mem-stash:
          keyName: TYPEORM_PORT
      - run:
          name: Deploy backend
          command: |
            source $BASH_ENV
            cp -R backend .circleci/ansible/roles/deploy-backend/files
            envsubst < .circleci/ansible/roles/deploy-backend/files/.env.template > .circleci/ansible/roles/deploy-backend/files/.env
            ansible-playbook -i .circleci/ansible/inventory.txt .circleci/ansible/deploy-backend.yml
          no_output_timeout: 20m
      - destroy-environment:
          workflowId: ${CIRCLE_WORKFLOW_ID:0:7}
      - revert-migrations
      - slack/notify: *slack-notify-job-fail

  smoke-test:
    <<: *nodejs-image
    steps:
      - checkout
      - install-aws-cli
      - retrieve-key-from-mem-stash:
          keyName: BACKEND_PUBLIC_DNS_NAME
      - run:
          name: Backend smoke test.
          command: |
            source $BASH_ENV
            if curl -s --head "http://${BACKEND_PUBLIC_DNS_NAME}:3030/api/status"
            then
              echo "API is running on $BACKEND_PUBLIC_DNS_NAME:3030"
              exit 0
            else
              echo "API cannot be reached!"
              exit 1
            fi
      - run:
          name: Frontend smoke test.
          command: |
            export FRONTEND_PUBLIC_DNS_NAME="http://s3-bucket-udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website-us-east-1.amazonaws.com"
            if curl -s ${FRONTEND_PUBLIC_DNS_NAME} | grep "Welcome"
            then
              echo "Front-end is running on $FRONTEND_PUBLIC_DNS_NAME"
              exit 0
            else
              echo "Front-end cannot be reached!"
              exit 1
            fi
      - destroy-environment:
          workflowId: ${CIRCLE_WORKFLOW_ID:0:7}
      - revert-migrations
      - slack/notify: *slack-notify-job-fail

  prometheus-smoke-test:
    <<: *nodejs-image
    steps:
      - checkout
      - install-aws-cli
      - retrieve-key-from-mem-stash:
          keyName: PROMETHEUS_PUBLIC_DNS_NAME
      - run:
          name: Prometheus smoke test
          command: |
            source $BASH_ENV
            if curl -s --head "http://${PROMETHEUS_PUBLIC_DNS_NAME}:9090/"
            then
              echo "Server is running on $PROMETHEUS_PUBLIC_DNS_NAME:9090"
              return 0
            else
              echo "Server cannot be reached!"
              return 1
            fi

  cloudfront-update:
    <<: *aws-cli
    steps:
      - checkout
      - install-dependencies
      - run:
          name: Save previous workflow ID
          command: |
            touch workflow_id.txt
            aws cloudformation \
            list-exports --query "Exports[?Name==\`WorkflowID\`].Value" \
            --no-paginate --output text >> workflow_id.txt
      - save-key-to-mem-stash:
          keyName: PREVIOUS_WORKFLOW_ID
          keyValue: $(cat workflow_id.txt)
      - run:
          name: Ensure cloudfront infrastructure exists and it's up to date
          command: |
            aws cloudformation deploy \
            --template-file .circleci/files/cloudfront.yml \
            --stack-name udapeople-cloudfront \
            --parameter-overrides ID=${CIRCLE_WORKFLOW_ID:0:7}
          no_output_timeout: 20m
      - run:
          name: Create invalidation
          command: |
            aws cloudformation \
            list-exports --query "Exports[?Name==\`DistributionID\`].Value" \
            --no-paginate --output text >> distribution_id.txt
            export DISTRIBUTION_ID=$(cat distribution_id.txt)
            aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID --paths "/*"
          when: on_success
      - destroy-environment:
          workflowId: ${CIRCLE_WORKFLOW_ID:0:7}
      - revert-migrations
      - slack/notify: *slack-notify-job-fail

  cleanup:
    <<: *aws-cli
    steps:
      - install-dependencies
      - retrieve-key-from-mem-stash:
          keyName: PREVIOUS_WORKFLOW_ID
      - run:
          name: Delete old stack
          command: |
            source $BASH_ENV
            export BUCKET_EXISTS=$(aws s3 ls s3://s3-bucket-udapeople-${PREVIOUS_WORKFLOW_ID} 2>&1 | grep -c 'index.html')
            if [ "${PREVIOUS_WORKFLOW_ID}" != "${CIRCLE_WORKFLOW_ID:0:7}" ]
            then
              if [ $BUCKET_EXISTS -eq 1 ]
              then
                aws s3 rm s3://s3-bucket-udapeople-${PREVIOUS_WORKFLOW_ID} --recursive
              fi
              aws cloudformation delete-stack --stack-name udapeople-backend-${PREVIOUS_WORKFLOW_ID}
              aws cloudformation delete-stack --stack-name udapeople-frontend-${PREVIOUS_WORKFLOW_ID}
              echo "Previous resources have been deleted"
            else
              echo "No previous resources to be deleted"
            fi
      - destroy-environment:
          workflowId: ${CIRCLE_WORKFLOW_ID:0:7}
      - revert-migrations
      - slack/notify: *slack-notify-job-fail
      - slack/notify: *slack-notify-deploy-success
            

workflows:
  default:
    jobs:
      - build-frontend
      - build-backend
      - test-frontend:
          requires: [build-frontend]
      - test-backend:
          requires: [build-backend]
      - scan-backend:
          requires: [build-backend]
      - scan-frontend:
          requires: [build-frontend]
      - deploy-prometheus:
          filters:
            branches:
              only: [master]
      - deploy-infrastructure:
          requires: [test-frontend, test-backend, scan-frontend, scan-backend]
          filters:
            branches:
              only: [master]
      - configure-infrastructure:
          requires: [deploy-infrastructure]
      - configure-prometheus:
          requires: [deploy-prometheus]
      - run-migrations:
          requires: [configure-infrastructure]
      - deploy-frontend:
          requires: [run-migrations]
      - deploy-backend:
          requires: [run-migrations]
      - smoke-test:
          requires: [deploy-backend, deploy-frontend]
      - prometheus-smoke-test:
          requires: [configure-prometheus]
      - cloudfront-update:
          requires: [smoke-test]
      - cleanup:
          requires: [cloudfront-update]